{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T02:46:40.686332Z",
     "start_time": "2020-12-19T02:46:36.563356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:50:38.490845Z",
     "start_time": "2021-04-17T12:50:38.476866Z"
    }
   },
   "source": [
    "#  initialize the working environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  set the working directory & import some library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = \"####\"\n",
    "os.chdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    import pandas as pd\n",
    "    import numpy as np \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import confusion_matrix \n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    %matplotlib inline\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    import tensorflow as  tf\n",
    "    from tensorflow.random import set_seed\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation\n",
    "    from keras.optimizers import SGD\n",
    "    import os \n",
    "    import random\n",
    "    from tensorflow.keras import regularizers, models, layers\n",
    "    from keras.callbacks import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T13:23:21.529195Z",
     "start_time": "2021-04-17T13:23:21.517227Z"
    }
   },
   "source": [
    "## some customize function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  plot function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     39,
     49,
     70,
     102,
     146,
     169,
     175,
     204,
     234
    ]
   },
   "outputs": [],
   "source": [
    "def def_auc_micro(y_test,temp3,file_name,nb_classes = 3):\n",
    "    from itertools import cycle\n",
    "    from scipy import interp\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import auc\n",
    "    import matplotlib.pyplot as plt\n",
    "    Y_valid=y_test\n",
    "    Y_pred=temp3\n",
    "    # Binarize the output\n",
    "    Y_valid = label_binarize(Y_valid, classes=[i for i in range(nb_classes)])\n",
    "    plt.rc('font',family='Times New Roman') \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_valid.ravel(), Y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    lw = 2\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "      label='micro-average ROC curve (area = {0:0.4f})'\n",
    "      ''.format(roc_auc[\"micro\"]),\n",
    "      color='cornflowerblue', linestyle='-', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(False)\n",
    "    plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show();\n",
    "def plot_confusion(con_mat,file_name,label):\n",
    "    import seaborn as sns\n",
    "    #plt.rc('font',family='Times New Roman') \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(con_mat, annot=True, cmap='Blues',xticklabels=label , yticklabels= label)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    \n",
    "    plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show();\n",
    "def plot_precison_loss(history,file_name):\n",
    "    #plt.rc('font',family='Times New Roman') \n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict[\"loss\"]\n",
    "    val_loss_values = history_dict[\"val_loss\"]\n",
    "    accuracy_values = history_dict[\"accuracy\"]\n",
    "    val_accuracy_values = history_dict[\"val_accuracy\"]\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "    \n",
    "    plt.plot(epochs, accuracy_values, '#40bad5', alpha=0.75, label='Training_precision')\n",
    "    plt.plot(epochs, val_accuracy_values, '#ff5f40', alpha=0.75, label='Validate_precision')\n",
    "    plt.plot(epochs, val_loss_values, '#e05297', alpha=0.75, label='Training_loss')\n",
    "    plt.plot(epochs, loss_values, '#222831', alpha=0.75, label='Validate_loss')\n",
    "    \n",
    "    plt.title('Loss:Fitting on dataset')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "#     plt.savefig(group + '_loss_.pdf', bbox_inches='tight')\n",
    "    plt.grid(False)\n",
    "    plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show();\n",
    "def plot_importance(df_temp,if_save=False,file_name=None):\n",
    "    df=df_temp.copy()\n",
    "    x = df.loc[:, 'importance']\n",
    "    df['mpg_z'] = x\n",
    "    df['colors'] = ['red' if x < 0 else 'darkgreen' for x in df['mpg_z']]\n",
    "    df.sort_values('mpg_z', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Draw plot\n",
    "    plt.figure(figsize=(7,8), dpi= 80)\n",
    "    plt.scatter(df.mpg_z, df.index, s=800, alpha=.6, color=df.colors)\n",
    "    for x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):\n",
    "        t = plt.text(x, y, round(tex, 2), horizontalalignment='center', \n",
    "                     verticalalignment='center', fontdict={'color':'white','size':15})\n",
    "\n",
    "    # Decorations\n",
    "    # Lighten borders\n",
    "    plt.gca().spines[\"top\"].set_alpha(.3)\n",
    "    plt.gca().spines[\"bottom\"].set_alpha(.3)\n",
    "    plt.gca().spines[\"right\"].set_alpha(.3)\n",
    "    plt.gca().spines[\"left\"].set_alpha(.3)\n",
    "\n",
    "    plt.yticks(range(len(df.index)), df['Gene Symbol'])\n",
    "    plt.title('Importance of Features', fontdict={'size':20})\n",
    "    plt.xlabel('Coefficient',fontdict={'size':15})\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.xlim(-0, 2.5)\n",
    "    \n",
    "    plt.tick_params(labelsize=15)\n",
    "    if (if_save ==True):\n",
    "        plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show()\n",
    "def plot_varience(df,cutoff=18,color='#0078b1',if_save = False,file_name =None):\n",
    "    sit = np.array(df.std() > cutoff)\n",
    "    ids = df.loc[:, sit].columns\n",
    "    var = df.std()\n",
    "    var = pd.DataFrame({'values': var})\n",
    "    var = var.sort_values(by='values', ascending = True)\n",
    "    y = var['values']\n",
    "    importance_index = np.min(np.where(var['values'] > cutoff))\n",
    "    importance_index\n",
    "\n",
    "    x = range(1, len(y) + 1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    ax.fill_between(x, 0, y, alpha=0.35)\n",
    "    ax.plot(x, y, 'r-', lw=2, c='#0078b1', alpha=0.8)\n",
    "    plt.ylabel(\"Standard deviation\", size=14)\n",
    "    plt.title('Control MCI AD', size=16)\n",
    "    ax.annotate(\"Cutoff\",\n",
    "                xy=(importance_index, 1),\n",
    "                xytext=(importance_index-400, 150),\n",
    "                color=\"black\",\n",
    "                weight=\"bold\",\n",
    "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color=\"black\"))\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(\"Features\", size=14)\n",
    "\n",
    "    #  Draw a subgraph\n",
    "    axins = ax.inset_axes((0.3, 0.35, 0.55, 0.45))\n",
    "    axins.plot(x, y, 'r-', lw=2, c=color, alpha=0.8)\n",
    "    axins.fill_between(x, 0, y, alpha=0.35)\n",
    "    axins.annotate(\"Cutoff\",\n",
    "                   xy=(importance_index, cutoff),\n",
    "                   xytext=(importance_index+2, cutoff+4),\n",
    "                   color=\"black\",\n",
    "                   weight=\"bold\",\n",
    "                   arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color=\"black\"))\n",
    "\n",
    "    # Adjust the display range of the subcoordinate system\n",
    "    axins.set_xlim(importance_index-50, importance_index+50)\n",
    "    axins.set_ylim(cutoff-10, cutoff+10)\n",
    "    #plt.savefig('plot/GSE63060_filter_std.pdf')\n",
    "    if (if_save ==True):\n",
    "        plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "def plot_corr_status(df_temp,if_save=False,file_name=None):\n",
    "    df = df_temp.copy()\n",
    "    x = df.loc[:, 'corr']\n",
    "    df['mpg_z'] = x\n",
    "    df['colors'] = ['red' if x < 0 else 'darkgreen' for x in df['mpg_z']]\n",
    "    df.sort_values('mpg_z', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    plt.figure(figsize=(14,10), dpi= 80)\n",
    "    plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4, linewidth=5)\n",
    "\n",
    "    # Decorations\n",
    "    plt.gca().set(ylabel='', xlabel='Coefficient')\n",
    "    \n",
    "    plt.yticks(range(len(df.index)), df['Gene Symbol'])\n",
    "    plt.xlabel('Coefficient',fontdict={'size':20})\n",
    "    \n",
    "    text = 'Correlation index of ' +  str(data_corr.shape[0]) + ' features to disease status'\n",
    "    plt.title(label = text , fontdict={'size':20})\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.tick_params(labelsize=15)\n",
    "    if (if_save ==True):\n",
    "        plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show()\n",
    "def randomcolor():\n",
    "    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
    "    color = \"\"\n",
    "    for i in range(6):\n",
    "        color += colorArr[random.randint(0,14)]\n",
    "    return \"#\"+color\n",
    "def plot_pca(data, label,random_state = 0,if_save=False,file_name=None):\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    X = data.copy()\n",
    "    model = PCA(n_components=2,random_state=random_state ) \n",
    "    Y = model.fit_transform(X) \n",
    "    group_name = label.copy().replace([0, 1, 3,2],['CT', \"MCI\", 'PD','AD'])\n",
    "    group_color = label.copy().replace([0, 1, 3,2],[\"#1EB2A6\",\"#ffc4a3\",\"#949cdf\",\"#ff4646\"])#  绿色\n",
    "    \n",
    "    temp = pd.DataFrame({'x':Y[:,0],\n",
    "            'y':Y[:,1],\n",
    "           'g':group_name.values.reshape(-1,),\n",
    "                    'c':group_color.values.reshape(-1,)\n",
    "                        }\n",
    "            )\n",
    "    # Draw Plot for Each Category\n",
    "    plt.figure(figsize=(8, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    for category in np.unique(group_name):\n",
    "        plt.scatter('x', 'y', \n",
    "                data=temp.loc[temp.g==category, :], \n",
    "                s=20, c=color[category], label=str(category))\n",
    "    plt.xticks(fontsize=12); plt.yticks(fontsize=12)\n",
    "    plt.title(\"PCA\", fontsize=22)\n",
    "    plt.legend(fontsize=12) \n",
    "    plt.grid(False)\n",
    "    if (if_save ==True):\n",
    "        plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    \n",
    "    plt.show(); \n",
    "def plot_tsne(data, label,random_state = 0,if_save=False,file_name=None):\n",
    "    import numpy as np\n",
    "    from sklearn.manifold import TSNE\n",
    "    X = data.copy()\n",
    "    model = TSNE(random_state=random_state ) \n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = model.fit_transform(X) \n",
    "    group_name = label.copy().replace([0, 1, 3,2],['CT', \"MCI\", 'PD','AD'])\n",
    "    group_color = label.copy().replace([0, 1, 3,2],[\"#1EB2A6\",\"#ffc4a3\",\"#949cdf\",\"#ff4646\"])#  绿色\n",
    "    label_name = {'CT':\"CT(n=156)\", \"MCI\":\"MCI (n=50)\", 'PD':\"PD (n=132)\",'AD':\"AD (n=50)\"}\n",
    "    temp = pd.DataFrame({'x':Y[:,0],\n",
    "            'y':Y[:,1],\n",
    "           'g':group_name.values.reshape(-1,),\n",
    "                    'c':group_color.values.reshape(-1,)\n",
    "                        }\n",
    "            )\n",
    "    # Draw Plot for Each Category\n",
    "    plt.figure(figsize=(8, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    for category in np.unique(group_name):\n",
    "        plt.scatter('x', 'y', \n",
    "                data=temp.loc[temp.g==category, :], \n",
    "                s=20, c=color[category], label=label_name[category])\n",
    "    plt.xticks(fontsize=12); plt.yticks(fontsize=12)\n",
    "    plt.title(\"TSNE\", fontsize=22)\n",
    "    plt.legend(fontsize=12) \n",
    "    plt.grid(False)\n",
    "    if (if_save ==True):\n",
    "        plt.savefig('E://文章//pic//'+file_name+'.pdf', bbox_inches='tight',dpi=300)\n",
    "    plt.show()\n",
    "    plt.show(); \n",
    "def plot_corr(X_new1,if_save=False,file_name=None):\n",
    "    corr_data = X_new1.copy()\n",
    "    dcorr = corr_data.corr(method='pearson')\n",
    "    dcorr = round(dcorr, 2)\n",
    "    from string import ascii_letters\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import palettable\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(11, 9),dpi=100)\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(data=dcorr,cmap=cmap,annot=True)\n",
    "    if (if_save ==True):\n",
    "        plt.savefig('E://文章//pic//'+file_name+'.tiff', bbox_inches='tight',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     14,
     41,
     62
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Arial') # \n",
    "\n",
    "# random seed for global envirment \n",
    "def seed_tensorflow(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_seed(seed)\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all' \n",
    "\n",
    "# substract the most N importanr feature in model \n",
    "def importance_n(temp,feature_keep = 10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    temp : data.frame\n",
    "        DESCRIPTION.\n",
    "    feature_keep : TYPE, optional\n",
    "        DESCRIPTION. The default is 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_new2 : the new dataframe shift by importance based on feature_keep \n",
    "        DESCRIPTION.\n",
    "    \"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    from sklearn.feature_selection import RFE\n",
    "    estimator  =  SVC(kernel = \"linear\")\n",
    "    selector  =  RFE(estimator, step = 1,n_features_to_select = feature_keep)\n",
    "    x_df  =  temp.copy()\n",
    "    y_df  =  np.ravel(label.copy())\n",
    "    selector  =  selector.fit(x_df, y_df.astype('int'))\n",
    "    X_new2  =  x_df.iloc[:,selector.support_]\n",
    "    return X_new2\n",
    "\n",
    "# create new feature \n",
    "def create_new_feature(df,X_new):\n",
    "    df1=df.loc[:,X_new.columns]\n",
    "    a=pd.DataFrame()\n",
    "    for i in range(df1.shape[1]-1):\n",
    "        a=pd.concat([a,pd.DataFrame(np.array(df1.iloc[:,i])/\n",
    "        np.array(df1.iloc[:,(i+1):df1.shape[1]]).T)],axis=0) \n",
    "    a=a.T\n",
    "    temp=a\n",
    "    temp.columns=range(int(X_new.shape[1]*(X_new.shape[1]-1)/2.0))\n",
    "    temp.index=X_new.index\n",
    "    data_all1 = standare_data(temp)\n",
    "    temp=data_all1.copy()\n",
    "    temp1=[]\n",
    "    for i in range(X_new.shape[1]):\n",
    "        for j in range(i+1,X_new.shape[1]):\n",
    "            tempij=X_new.columns[i]+'/'+X_new.columns[j]\n",
    "            temp1.append(tempij)\n",
    "    temp.columns=temp1\n",
    "    return temp\n",
    "\n",
    "# basic ann model \n",
    "def build_model(input_dims,activation_relu=\"relu\",dropout_num=0,optimizer='adam',l1=0,l2=0,finally_layer=3):\n",
    "\n",
    "#                                                     keras.initializers.RandomNormal(mean=0.0, stddev=0.05)\n",
    "                                                    keras.initializers.RandomNormal(mean=0.0, stddev=0.05)\n",
    "                                                    model = Sequential()\n",
    "\n",
    "                                                    model.add(Dense(layers1, activation=activation_relu,kernel_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "                                                    model.add(Dropout(dropout_num))\n",
    "                                                    keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "                                                    model.add(Dense(layers2, activation=activation_relu,kernel_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "                                                    model.add(Dropout(dropout_num))\n",
    "                                                    keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "                                                    model.add(Dense(layers3, activation=activation_relu,kernel_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "                                                    model.add(Dropout(dropout_num))\n",
    "                                                    keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "                                                    model.add(Dense(finally_layer, activation='softmax'))\n",
    "\n",
    "                                                    model.compile(optimizer=optimizer,\n",
    "                                                                  loss='categorical_crossentropy',\n",
    "                                                                  metrics=['accuracy'])\n",
    "                                                    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data  and   transform label to numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as numpy \n",
    "data = \"ct_ad_mci_exp_delsample.txt\"\n",
    "label = \"ct_ad_mci_label_delsample.txt\"\n",
    "degs = \"ct_ad_mci_uniondegs_delsample.txt\"\n",
    "data = pd.read_csv(data,header=0,sep=\"\\t\",index_col=0)\n",
    "label = pd.read_csv(label,header=0,sep=\"\\t\",index_col=None)\n",
    "degs = pd.read_csv(degs,header=0,sep=\"\\t\",index_col=0)\n",
    "df = data.loc[degs.iloc[:,0],]\n",
    "label = label.copy().replace(['CT', \"MCI\", 'PD','AD'], # 构建 label\n",
    "                                           [0, 1, 3,2]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:54:34.332416Z",
     "start_time": "2021-04-17T12:54:34.316457Z"
    }
   },
   "source": [
    "## feature engining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varience (delete the probe that varience is less than lower quartile of summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T12:12:47.368183Z",
     "start_time": "2021-04-07T12:12:47.343223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1879.000000\n",
       "mean       95.904896\n",
       "std       192.318229\n",
       "min         4.451984\n",
       "25%        17.993745\n",
       "50%        38.132601\n",
       "75%       101.959067\n",
       "max      2518.112544\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.T\n",
    "df.std().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sit = np.array(df.std() >18)\n",
    "df = df.loc[:,sit]\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "fitted_scaler = StandardScaler().fit_transform(df)\n",
    "fitted_scaler\n",
    "data_all= pd.DataFrame(fitted_scaler,\n",
    "                               columns=df.columns,index=df.index)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "y_df = label.copy() # copy \n",
    "fs = FeatureSelector(data = data_all, labels = y_df.astype('int'))\n",
    "fs.identify_missing(missing_threshold=0.6)\n",
    "fs.identify_single_unique()\n",
    "# result : none of missing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  colinearity ( threshold = 0.7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_collinear(correlation_threshold=0.70)\n",
    "correlated_features = fs.ops['collinear']\n",
    "# delete the probe with colinearity grate than 0.70 \n",
    "data_all = data_all.drop(columns =  correlated_features)\n",
    "data_all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T13:06:38.413678Z",
     "start_time": "2021-04-17T13:06:38.396723Z"
    }
   },
   "source": [
    "### importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "estimator = SVC(kernel=\"linear\")\n",
    "selector = RFE(estimator, step=1,n_features_to_select=20)\n",
    "x_df = data_all.copy()\n",
    "y_df = np.ravel(label.copy())\n",
    "selector = selector.fit(x_df, y_df.astype('int'))\n",
    "X_new = x_df.iloc[:,selector.support_]\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T13:10:30.873427Z",
     "start_time": "2021-04-17T13:10:30.857442Z"
    }
   },
   "source": [
    "## different classfier including RF(random forest ), SVM (support vector machine ), NB (naive bayes ), DT (decision  tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T12:15:57.613094Z",
     "start_time": "2021-04-07T12:15:56.768353Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\software\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "<ipython-input-7-8d166fbab884>:74: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  clf.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9803921568627451\n",
      "NB , 0.8269230769230769 ,0.9101331360946745\n",
      "0.9803921568627451\n",
      "KNN , 0.8846153846153846 ,0.9800295857988165\n",
      "1.0\n",
      "DT , 0.9615384615384616 ,0.9711538461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-8d166fbab884>:74: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "RF , 0.9807692307692307 ,0.9994452662721893\n"
     ]
    }
   ],
   "source": [
    "# input :  data  and label \n",
    "# output : roc and accuracy \n",
    "X_new = df.copy()\n",
    "y_df = label.copy()\n",
    "if True:\n",
    "    import os\n",
    "    import sys\n",
    "    import warnings\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import csv\n",
    "    import argparse\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "    from sklearn.model_selection import GridSearchCV \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "    from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "    from sklearn.utils import class_weight\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from sklearn.svm import LinearSVC\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "    from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, chi2\n",
    "    from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif\n",
    "\n",
    "    import seaborn as sns; sns.set() # data visualization library \n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "    from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "    import joblib\n",
    "    \n",
    "    def MLOuterCV(Xdata, Ydata, label = 'my', seed = 74):\n",
    "        X=Xdata.copy()\n",
    "        y=Ydata.copy()\n",
    "        from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "        from sklearn.model_selection import StratifiedShuffleSplit\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        for train_index, test_index in sss.split(X, y):  \n",
    "          X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "          y_train, y_test = y.iloc[train_index,:], y.iloc[test_index,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        names = ['NB','KNN','DT','RF'] \n",
    "   \n",
    "        classifiers = [GaussianNB(),\n",
    "                       KNeighborsClassifier(3),\n",
    "                       DecisionTreeClassifier(random_state = seed),\n",
    "                       RandomForestClassifier(n_jobs=-1,random_state=seed)\n",
    "                      ]\n",
    "       \n",
    "        df_res = pd.DataFrame(columns=names)\n",
    "        \n",
    "        for name, clf in zip(names, classifiers):\n",
    "          \n",
    "            clf.fit(X_train,y_train)\n",
    "            Y_pred = clf.predict(X_test)\n",
    "            scores = accuracy_score(y_test,Y_pred)\n",
    "            df_res[name] = scores\n",
    "            y_scores=clf.predict_proba(X_test)\n",
    "            y_test_dummy= keras.utils.to_categorical(y_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test_dummy.ravel(), y_scores.ravel())\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print('%s , %s ,%s'  %(name,scores,roc_auc))\n",
    "        return df_res,fpr,tpr,roc_auc\n",
    "seed=2\n",
    "df_res,fpr,tpr,roc_auc = MLOuterCV(X_new,y_df, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T13:14:30.036533Z",
     "start_time": "2021-04-17T13:14:30.030547Z"
    }
   },
   "source": [
    "## ANN model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T13:15:26.905116Z",
     "start_time": "2021-04-17T13:15:26.895155Z"
    }
   },
   "source": [
    "###  parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_relu = \"relu\"\n",
    "dropout_nums = [0.0]\n",
    "optimizers = ['adam']\n",
    "l1s = [0.00]\n",
    "l2s = [0.00]\n",
    "batch_size = 64\n",
    "epochs = [800]\n",
    "patiences = [40]\n",
    "layers1 = 50\n",
    "layers2 = 25\n",
    "layers3 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T13:19:04.265996Z",
     "start_time": "2021-04-17T13:19:04.253027Z"
    }
   },
   "outputs": [],
   "source": [
    "### you can use list such as dropout_nums = [0.0 ,0.1, 0.2 ]  in your parameter to choose the best combination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "                                    # data split to training ,validate ,test set \n",
    "                                    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "                                    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=randoms)\n",
    "                                    X=X_new.copy()\n",
    "                                    y=y_df.copy()\n",
    "                                    \n",
    "                                    for train_index, test_index in sss.split(X, y):  \n",
    "                                            X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "                                            y_train, y_test = y.iloc[train_index,:], y.iloc[test_index,:]\n",
    "                                    \n",
    "                                    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=randoms)\n",
    "                                    X=X_train.copy()\n",
    "                                    y=y_train.copy()\n",
    "                                    for train_index, test_index in sss.split(X, y):  \n",
    "                                            X_train, X_validate = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "                                            y_train, y_validate = y.iloc[train_index,:], y.iloc[test_index,:]\n",
    "                                    #  train\n",
    "                                    x_train=np.array(X_train)\n",
    "                                    y_train_dummy= keras.utils.to_categorical(y_train)\n",
    "                                    #  validate\n",
    "                                    x_validate=np.array(X_validate)\n",
    "                                    y_validate_dummy= keras.utils.to_categorical(y_validate)\n",
    "                                    #  test \n",
    "                                    x_test=np.array(X_test)\n",
    "                                    y_test_dummy= keras.utils.to_categorical(y_test)\n",
    "\n",
    "                                    input_dims=x_train.shape[1]\n",
    "\n",
    "                                    def build_model(input_dims,activation_relu=\"relu\",dropout_num=0,optimizer='adam',l1=0,l2=0):\n",
    "\n",
    "#                                                     keras.initializers.RandomNormal(mean=0.0, stddev=0.05)\n",
    "                                                    keras.initializers.RandomNormal(mean=0.0, stddev=0.05)\n",
    "                                                    model = Sequential()\n",
    "\n",
    "                                                    model.add(Dense(layers1, activation=activation_relu,kernel_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "                                                    model.add(Dropout(dropout_num))\n",
    "                                                    keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "                                                    model.add(Dense(layers2, activation=activation_relu,kernel_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "                                                    model.add(Dropout(dropout_num))\n",
    "                                                    keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "                                                    model.add(Dense(layers3, activation=activation_relu,kernel_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "                                                    model.add(Dropout(dropout_num))\n",
    "                                                    keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "                                                    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "                                                    model.compile(optimizer=optimizer,\n",
    "                                                                  loss='categorical_crossentropy',\n",
    "                                                                  metrics=['accuracy'])\n",
    "                                                    return model\n",
    "\n",
    "                                                            seed_tensorflow(i)\n",
    "                                                            from sklearn.metrics import confusion_matrix\n",
    "                                                            mlp_model= build_model(input_dims=input_dims,activation_relu=activation_relu,dropout_num=dropout_num,\n",
    "                                                                                   optimizer=optimizer,l1=l1,l2=l2)\n",
    "                                                            history=mlp_model.fit(x_train, y_train_dummy,epochs=epoch,validation_data=(x_validate,y_validate_dummy),\n",
    "                                                                                  verbose=0,batch_size=batch_size,\n",
    "                                                                                  callbacks=[EarlyStopping(monitor=\"val_loss\", patience=patience)])\n",
    "                                                                          #batch_size=128)\n",
    "                                                            history_dict = history.history\n",
    "                                                            \n",
    "                                                            # train\n",
    "                                                            loss,acc_train=mlp_model.evaluate(x_train,y_train_dummy,verbose=0)\n",
    "\n",
    "                                                            #  validate\n",
    "                                                            loss,acc_validate=mlp_model.evaluate(x_validate,y_validate_dummy,verbose=0)\n",
    "\n",
    "                                                            #  test \n",
    "                                                            loss,acc_test=mlp_model.evaluate(x_test,y_test_dummy,verbose=0)\n",
    "         \n",
    "                                                            acc_train\n",
    "                                                            acc_validate\n",
    "                                                            acc_test\n",
    "                                                            result = [batch_size,l1,l2,dropout_num,randoms,epoch,layers1,layers2,\n",
    "                                                                              i,acc_train,acc_validate,acc_test]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 371.33334,
   "position": {
    "height": "40px",
    "left": "1214px",
    "right": "20px",
    "top": "104px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
